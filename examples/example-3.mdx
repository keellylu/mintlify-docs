---
title: 'Example 3'
icon: 'book'
---

## Approach

This approach uses the `DenseCaptioningAgent` to generate a text description of each person. These descriptions inform the classification task.

### **Step 1. Import Overeasy Agents**

First, you need to import the necessary components from the Overeasy library, including the Workflow class and any Agents you plan to use. 

```python example_3.py
from overeasy import Workflow, BoundingBoxSelectAgent, DenseCaptioningAgent, SplitAgent, InstructorTextAgent, JoinAgent, ToClassificationAgent
```

### **Step 2. Load the Image**

Load the image you want to process. The code below constructs a path to an image named `"construction.jpg"` located in the same directory. It loads this image using Pillow.

```python example_3.py
from PIL import Image

image = Image.open("./examples/construction_workers.jpg")
```

### **Step 3. Initialize Workflow**

Instantiate the `Workflow` class with a list of Overeasy Agents, where the order in the list defines the sequence of operations.

```python example_3.py
workflow = Workflow([
    BoundingBoxSelectAgent(classes=["person"]),
    NMSAgent(iou_threshold=0.5, score_threshold=0),
    SplitAgent(),
    DenseCaptioningAgent(),
    InstructorTextAgent(response_model=PPE),
    ToClassificationAgent(fn=lambda x: "has ppe" if x.hardhat else "no ppe"),
    JoinAgent()
])
```

> Let's break down the Agents in this Workflow.
> 
> 1. **`BoundingBoxSelectAgent`:** This Agent is used to detect and select bounding boxes around objects in the image that are classified as "person".
> 
> 2. **`NMSAgent`:** This Agent performs Non-Maximum Suppression (NMS) with an Intersection over Union (IoU) threshold of 0.5 and a score threshold of 0. This means it keeps the bounding box with the highest score and removes any other box that overlaps it by more than 50%.
> 
> 3. **`SplitAgent`:** This Agent splits the input into multiple parts based on bounding boxes identified in the previous step. Each split now represents an individual person detected in the image.
> 
> 4. **`DenseCaptioningAgent`:** This Agent generates dense captions for the image regions that describe the image content.
> 
> 5. **`InstructorTextAgent`:** This Agent uses the PPE model to interpret the generated image captions to determine if a hard hat is present. 
> 
> 6. **`ToClassificationAgent`:** This Agent converts the PPE output into a classification label with two classes: "has ppe" or "no ppe".
> 
> 6. **`JoinAgent`:** This Agent aggregates results from previous steps into a single output.

### **Step 4. Execute the Workflow**

Call the `execute` method of your Workflow with an initial image. This will process the image through all the Agents.

```python example_3.py
result, graph = workflow.execute(image)
```

### **Step 5. Visualize Results**

Finally, visualize the results to see the classification and detection outputs. Use the `visualize` method to view a graphical representation of the Workflow execution, including intermediate images and data at each step.

```python example_3.py
workflow.visualize(graph)
```

Here's what the output looks like:

### Full Code

```python example_3.py
from PIL import Image
import streamlit as st
from overeasy import Workflow, BoundingBoxSelectAgent, DenseCaptioningAgent, SplitAgent, InstructorTextAgent, JoinAgent, ToClassificationAgent
from overeasy.models.detection import NMSAgent
from pydantic import BaseModel

image_path = "/home/azureuser/overeasy-internal/examples/construction.jpg"
image = Image.open(image_path)

class PPE(BaseModel):
    hardhat: bool

workflow = Workflow([
    BoundingBoxSelectAgent(classes=["person"]),
    NMSAgent(iou_threshold=0.5, score_threshold=0),
    SplitAgent(),
    DenseCaptioningAgent(),
    InstructorTextAgent(response_model=PPE),
    ToClassificationAgent(fn=lambda x: "has ppe" if x.hardhat else "no ppe"),
    JoinAgent()
])

result, graph = workflow.execute(image)

workflow.visualize(graph)
```