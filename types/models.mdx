---
title: '⚙️ Models'
description: ''
---

## Overview
### Bounding Box Detection Models

Overeasy supports a variety of models that specialize in bounding box object detection.

<ParamField path="DETIC()">
    **DETIC (DEtection Transformer with Improved Context)** is a computer vision model designed for object detection that leverages transformer-based architecture for improved accuracy and context understanding.

    You can find more information on DETIC [here]().
</ParamField>

<ParamField path="GroundingDINO()">
    **Grounding DINO (Detection with Implicit Neural Objectness)** is an object detection model that uses implicit neural representations to identify and localize objects without explicit bounding boxes.

    You can find more information on Grouding DINO [here]().
</ParamField>

<ParamField path="OwlV2()">
    **OwlV2 (Open-World Language-guided Visual representation)** is a model that combines natural language processing with computer vision to enable open-world understanding and object detection based on textual descriptions.

    You can find more information on OwlV2 [here]().
</ParamField>

<ParamField path="YOLOWorld()">
    **YOLO-world (You Only Look Once)** is an advanced version of the YOLO object detection model, designed for real-time processing and capable of recognizing a wide range of objects in diverse environments.

    You can find more information on YOLOWorld [here]().
</ParamField>

### LLMs

<ParamField path="GPT4V()">
    **YOLO-world (You Only Look Once)** is an advanced version of the YOLO object detection model, designed for real-time processing and capable of recognizing a wide range of objects in diverse environments.

    You can find more information on YOLOWorld [here]().
</ParamField>

<ParamField path="QwenVL()">
    **YOLO-world (You Only Look Once)** is an advanced version of the YOLO object detection model, designed for real-time processing and capable of recognizing a wide range of objects in diverse environments.

    You can find more information on YOLOWorld [here]().
</ParamField>


A multimodal LLM capable of handling both text and visual inputs, useful for tasks that require understanding and generating content based on combined input types.