---
title: 'Vision Prompt Agent'
description: 'The `VisionPromptAgent` handles image-based queries.'
icon: 'robot'
---

## Initialization
### Parameters

The `VisionPromptAgent` is initialized with two arguments: 

```
VisionPromptAgent(query, model)
```

<ResponseField name="query" type="string" required>
**The prompt used to analyze the image.**

Here are a few illustrative examples: 
>
>Given an image of an X-ray scan, `query = "Is there a fracture in the bone?"`
>
>Given a frame of security camera footage, `query = "Identify any suspicious activities or individuals in this security camera footage."`
>
>Given a photo of a manufactured product, `query = "Inspect this product for any defects or irregularities."`

</ResponseField>

<ResponseField name="model" type="MultimodalLLM" required>
  **The selected model.** All supported `MultimodalLLM` models can be found below: 
  <Expandable title="Supported MultimodalLLMs">
    <ResponseField name="GPT4Vision()" type="MultimodalLLM (Default)">
      Supports `gpt-4-turbo` , `gpt-4o` .
    </ResponseField>
    <ResponseField name="Claude()" type="MultimodalLLM">
      Supports `claude-3-opus-20240229` , `claude-3-haiku-20240307` , `claude-3-sonnet-20240229` .
    </ResponseField>
    <ResponseField name="Gemini()" type="MultimodalLLM">
      Supports `gemini-pro-vision` .
    </ResponseField>
    <ResponseField name="QwenVL()" type="MultimodalLLM">
      Supports `qwen-vl-chat` .
    </ResponseField>

  </Expandable>
</ResponseField>

## Example
Here is an example of the `VisionPromptAgent` designed for a Workflow to detect damage on an egg.

{/* <Frame>
![Egg](/agents/image-agents/images/cracked_egg.jpg)
</Frame> */}

```python example.py
VisionPromptAgent("Is the egg damaged in anyway?",  model=GPT4Vision())
```
{/* 
## Diagram

```mermaid
%%{init: {'theme': 'neutral' } }%%
flowchart TB
    direction TB
    subgraph NODE1
        direction LR
        I1("IMAGE <img src='/agents/image-agents/images/cracked_egg.jpg';' />") ~~~ D1(DATA)
    end
    subgraph NODE2
        direction LR
        I2("IMAGE <img src='/agents/image-agents/images/cracked_egg.jpg';' />") ~~~ D2("DATA\n "&quotYes, the egg in the image is damaged. The shell\n is cracked, and the contents of the egg have spilled out.&quot"")
    end
    NODE1 --VisionPromptAgent--> NODE2
``` */}

{/* <CardGroup cols={2}>
  <Card
    title="Egg Carton Example"
    icon="circle-info"
    href="https://overeasy.sh/docs/examples/egg-carton-example"
  >
    See Full Example
  </Card>
</CardGroup> */}

{/* ## Use Cases
The `VisionPromptAgent` is designed to handle use cases where an image needs to be analyzed or interpreted in the context of a specific textual query:

> - Image Captioning
> - Contextual Questions
> - Sentiment Analysis
> - Image-based Fact Verification
> - Search + Retrieval on Images */}